{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read patients from file, len(patients): 17612\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 174\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xs, ys, times, lens, masks\n\u001b[0;32m--> 174\u001b[0m res_train \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43my_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOutcome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat_mimic4_ehr.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m res_val \u001b[38;5;241m=\u001b[39m process_subset(data_path, flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m                          y_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m, origin_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat_mimic4_ehr.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    178\u001b[0m res_test \u001b[38;5;241m=\u001b[39m process_subset(data_path, flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m                           y_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m, origin_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat_mimic4_ehr.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mprocess_subset\u001b[0;34m(data_path, origin_filename, y_feature, flag, save, shuffle_patients)\u001b[0m\n\u001b[1;32m    123\u001b[0m lens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m masks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 125\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatas\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime_feature_origin\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# times len=data_len and freq=1h\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/groupby/ops.py:790\u001b[0m, in \u001b[0;36mBaseGrouper.get_iterator\u001b[0;34m(self, data, axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(data, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    789\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys_seq\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, splitter)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/groupby/ops.py:1322\u001b[0m, in \u001b[0;36mDataSplitter.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1322\u001b[0m     sdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msorted_data\u001b[49m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1325\u001b[0m         \u001b[38;5;66;03m# we are inside a generator, rather than raise StopIteration\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;66;03m# we merely return signal the end\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/_libs/properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/groupby/ops.py:1336\u001b[0m, in \u001b[0;36mDataSplitter.sorted_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msorted_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/generic.py:3871\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3862\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3863\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_copy is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m always returns a copy, so there is no need to specify this.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3865\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   3866\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   3867\u001b[0m     )\n\u001b[1;32m   3869\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_take((), kwargs)\n\u001b[0;32m-> 3871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3880\u001b[0m \u001b[38;5;124;03mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[1;32m   3881\u001b[0m \n\u001b[1;32m   3882\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m-> 3886\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3888\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3891\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/internals/managers.py:977\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_indices:\n\u001b[1;32m    975\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m--> 977\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    979\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[1;32m    980\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    983\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/indexes/base.py:1183\u001b[0m, in \u001b[0;36mIndex.take\u001b[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1183\u001b[0m     taken \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_na_value\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# algos.take passes 'axis' keyword which not all EAs accept\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m     taken \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m   1189\u001b[0m         indices, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_na_value\n\u001b[1;32m   1190\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.11/site-packages/pandas/core/algorithms.py:1577\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     result \u001b[38;5;241m=\u001b[39m take_nd(\n\u001b[1;32m   1573\u001b[0m         arr, indices, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m   1574\u001b[0m     )\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1576\u001b[0m     \u001b[38;5;66;03m# NumPy style\u001b[39;00m\n\u001b[0;32m-> 1577\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data_path = './processed_data/'\n",
    "# data_path = os.path.join(data_path, 'format_mimic4_ehr.csv')\n",
    "\n",
    "\n",
    "def process_subset(data_path, origin_filename='format_mimic4_ehr.csv', y_feature='Outcome', flag=\"train\", save=True, shuffle_patients=False):\n",
    "    df_raw = pd.read_csv(os.path.join(data_path, origin_filename))\n",
    "    # x_features = [\"Sex\",\"Age\"]+[\"Diastolic blood pressure\", \"Fraction inspired oxygen\", \"Glucose\",\n",
    "    # \"Heart Rate\", \"Height\", \"Mean blood pressure\", \"Oxygen saturation\",\n",
    "    # \"Respiratory rate\", \"Systolic blood pressure\", \"Temperature\", \"Weight\", \"pH\"] + [\"Capillary refill rate\", \"Glascow coma scale eye opening\",\n",
    "    # \"Glascow coma scale motor response\", \"Glascow coma scale total\", \"Glascow coma scale verbal response\"]\n",
    "    # x_features = df_raw.columns[df_raw.columns.str.contains('|'.join(x_features))].tolist()\n",
    "\n",
    "    basic_records = ['PatientID', 'RecordTime',\n",
    "                     'AdmissionTime', 'DischargeTime']\n",
    "    target_features = ['Outcome', 'LOS', 'Readmission']\n",
    "    # Sex and ICUType are binary features, others are continuous features\n",
    "    demographic_features = ['Sex', 'Age']\n",
    "    labtest_features = ['Capillary refill rate->0.0', 'Capillary refill rate->1.0',\n",
    "                        'Glascow coma scale eye opening->To Pain',\n",
    "                        'Glascow coma scale eye opening->3 To speech',\n",
    "                        'Glascow coma scale eye opening->1 No Response',\n",
    "                        'Glascow coma scale eye opening->4 Spontaneously',\n",
    "                        'Glascow coma scale eye opening->None',\n",
    "                        'Glascow coma scale eye opening->To Speech',\n",
    "                        'Glascow coma scale eye opening->Spontaneously',\n",
    "                        'Glascow coma scale eye opening->2 To pain',\n",
    "                        'Glascow coma scale motor response->1 No Response',\n",
    "                        'Glascow coma scale motor response->3 Abnorm flexion',\n",
    "                        'Glascow coma scale motor response->Abnormal extension',\n",
    "                        'Glascow coma scale motor response->No response',\n",
    "                        'Glascow coma scale motor response->4 Flex-withdraws',\n",
    "                        'Glascow coma scale motor response->Localizes Pain',\n",
    "                        'Glascow coma scale motor response->Flex-withdraws',\n",
    "                        'Glascow coma scale motor response->Obeys Commands',\n",
    "                        'Glascow coma scale motor response->Abnormal Flexion',\n",
    "                        'Glascow coma scale motor response->6 Obeys Commands',\n",
    "                        'Glascow coma scale motor response->5 Localizes Pain',\n",
    "                        'Glascow coma scale motor response->2 Abnorm extensn',\n",
    "                        'Glascow coma scale total->11', 'Glascow coma scale total->10',\n",
    "                        'Glascow coma scale total->13', 'Glascow coma scale total->12',\n",
    "                        'Glascow coma scale total->15', 'Glascow coma scale total->14',\n",
    "                        'Glascow coma scale total->3', 'Glascow coma scale total->5',\n",
    "                        'Glascow coma scale total->4', 'Glascow coma scale total->7',\n",
    "                        'Glascow coma scale total->6', 'Glascow coma scale total->9',\n",
    "                        'Glascow coma scale total->8',\n",
    "                        'Glascow coma scale verbal response->1 No Response',\n",
    "                        'Glascow coma scale verbal response->No Response',\n",
    "                        'Glascow coma scale verbal response->Confused',\n",
    "                        'Glascow coma scale verbal response->Inappropriate Words',\n",
    "                        'Glascow coma scale verbal response->Oriented',\n",
    "                        'Glascow coma scale verbal response->No Response-ETT',\n",
    "                        'Glascow coma scale verbal response->5 Oriented',\n",
    "                        'Glascow coma scale verbal response->Incomprehensible sounds',\n",
    "                        'Glascow coma scale verbal response->1.0 ET/Trach',\n",
    "                        'Glascow coma scale verbal response->4 Confused',\n",
    "                        'Glascow coma scale verbal response->2 Incomp sounds',\n",
    "                        'Glascow coma scale verbal response->3 Inapprop words',\n",
    "                        'Diastolic blood pressure', 'Fraction inspired oxygen', 'Glucose',\n",
    "                        'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation',\n",
    "                        'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight',\n",
    "                        'pH']\n",
    "\n",
    "    # set the features\n",
    "    x_features = demographic_features + labtest_features\n",
    "    normalize_features = ['Age'] + ['Diastolic blood pressure', 'Fraction inspired oxygen', 'Glucose',\n",
    "                                    'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation',\n",
    "                                    'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight',\n",
    "                                    'pH'] + ['LOS']\n",
    "    if y_feature != 'LOS':\n",
    "        normalize_features.remove('LOS')\n",
    "    nonormalize_features = list(set(x_features) - set(normalize_features))\n",
    "    y_feature = [y_feature]\n",
    "    time_feature_origin = ['DischargeTime']\n",
    "    id_feature = ['PatientID']\n",
    "    time_feature = ['Time']\n",
    "    df_raw = df_raw[x_features + y_feature + time_feature_origin + id_feature]\n",
    "\n",
    "    # group by patient\n",
    "    df_raw['Patient'] = df_raw['PatientID'].apply(lambda x: x.split('_')[0])\n",
    "    patients = df_raw['Patient'].unique()\n",
    "    patients_filename = os.path.join(data_path, 'patients.txt')\n",
    "    has_patients_file = os.path.exists(patients_filename)\n",
    "    if shuffle_patients or (not has_patients_file):\n",
    "        import random\n",
    "        random.shuffle(patients)\n",
    "        with open(patients_filename, 'w') as f:\n",
    "            for item in patients:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        print(f'shuffle patients and save to file')\n",
    "    else:\n",
    "        with open(patients_filename, 'r') as f:\n",
    "            patients = f.read().splitlines()\n",
    "        print(f'read patients from file, len(patients): {len(patients)}')\n",
    "\n",
    "    # seperate train, val, test 7, 1, 2\n",
    "    border1s = [0, int(len(patients) * 0.7), int(len(patients) * 0.8)]\n",
    "    border2s = [int(len(patients) * 0.7),\n",
    "                int(len(patients) * 0.8), len(patients)]\n",
    "    flag_id = {'train': 0, 'valid': 1, 'test': 2}\n",
    "    border1 = border1s[flag_id[flag]]\n",
    "    border2 = border2s[flag_id[flag]]\n",
    "    patients = patients[border1:border2]\n",
    "\n",
    "    train_data = df_raw[df_raw['Patient'].isin(patients)]\n",
    "    train_data = train_data[x_features]\n",
    "    scaler = StandardScaler()\n",
    "    train_data_need_normalize = train_data[normalize_features].values\n",
    "    scaler.fit(train_data_need_normalize)\n",
    "\n",
    "    # group by PatientID\n",
    "    df_data = df_raw[df_raw['Patient'].isin(patients)]\n",
    "    datas = df_data.groupby('PatientID')\n",
    "    # add time feature\n",
    "    xs = []\n",
    "    times = []\n",
    "    ys = []\n",
    "    lens = []\n",
    "    masks = []\n",
    "    for i, data in datas:\n",
    "        time_0 = data[time_feature_origin[0]].values[-1]\n",
    "        # times len=data_len and freq=1h\n",
    "        time_add = pd.Timestamp(\n",
    "            time_0) + pd.to_timedelta(np.arange(48) * 1, 'h')\n",
    "        time_add = time_features(\n",
    "            time_add, freq='h').transpose(1, 0)  # seq_len, 4\n",
    "        # scale and fillna\n",
    "        x_need_normalize = data[normalize_features]\n",
    "        x_no_normalize = data[nonormalize_features]\n",
    "        x_need_normalize = x_need_normalize.fillna(\n",
    "            method='ffill').fillna(method='bfill').fillna(0).values\n",
    "        x_no_normalize = x_no_normalize.fillna(\n",
    "            method='ffill').fillna(method='bfill').fillna(0).values\n",
    "        x_need_normalize = scaler.transform(x_need_normalize)\n",
    "        x = np.concatenate([x_need_normalize, x_no_normalize], axis=1)\n",
    "        # get the first 48 hours\n",
    "        len_x_origin = len(x)\n",
    "        x = x[:48]\n",
    "        if len(x) < 48:\n",
    "            x = np.concatenate([x, np.zeros((48-len(x), len(x[0])))], axis=0)\n",
    "            mask = np.concatenate([np.ones(len(x)), np.zeros(48-len(x))], axis=0)\n",
    "        else:\n",
    "            mask = np.ones(48)\n",
    "        y = data[y_feature].values[0]\n",
    "        xs.append(x)\n",
    "        ys.append(np.array([y]))\n",
    "        lens.append(min(len_x_origin, 48))\n",
    "        masks.append(mask)\n",
    "        times.append(time_add)\n",
    "\n",
    "    if save:\n",
    "        import pickle\n",
    "        pickle.dump(xs, open(os.path.join(data_path, f'{flag}_xs.pkl'), 'wb'))\n",
    "        pickle.dump(ys, open(os.path.join(data_path, f'{flag}_ys.pkl'), 'wb'))\n",
    "        pickle.dump(times, open(os.path.join(\n",
    "            data_path, f'{flag}_times.pkl'), 'wb'))\n",
    "        pickle.dump(lens, open(os.path.join(\n",
    "            data_path, f'{flag}_lens.pkl'), 'wb'))\n",
    "        pickle.dump(scaler, open(os.path.join(\n",
    "            data_path, f'{flag}_scaler.pkl'), 'wb'))\n",
    "        pickle.dump(masks, open(os.path.join(\n",
    "            data_path, f'{flag}_masks.pkl'), 'wb'))\n",
    "        \n",
    "        print(f'save {flag} data')\n",
    "    return xs, ys, times, lens, masks\n",
    "\n",
    "\n",
    "res_train = process_subset(data_path, flag='train', save=True,\n",
    "                           y_feature='Outcome', origin_filename='format_mimic4_ehr.csv')\n",
    "res_val = process_subset(data_path, flag='valid', save=True,\n",
    "                         y_feature='Outcome', origin_filename='format_mimic4_ehr.csv')\n",
    "res_test = process_subset(data_path, flag='test', save=True,\n",
    "                          y_feature='Outcome', origin_filename='format_mimic4_ehr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test, ys_test, times_test, lens_test, masks_test = res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(masks_test[0])\n",
    "ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
