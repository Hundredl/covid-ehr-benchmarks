{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data\n",
    "df_train: pd.DataFrame = pd.read_excel('./raw_data/time_series_375_prerpocess_en.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "- fill `patient_id`\n",
    "- add 2 new columns: total days in hospital (`TOT_DAY`), remaining days in hospital (`LOS`)\n",
    "- only reserve y-m-d for `RE_DATE` column\n",
    "- merge lab tests of the same (patient_id, date)\n",
    "- calculate and save features' statistics information (demographic and lab test data are calculated separately)\n",
    "- normalize data\n",
    "- feature selection\n",
    "- fill missing data (our filling strategy will be described below)\n",
    "- combine above data to time series data (one patient one record)\n",
    "- export to python pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill `patient_id` rows\n",
    "df_train['PATIENT_ID'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# add 2 new columns: total days in hospital (`TOT_DAY`), remaining days in hospital (`LOS`)\n",
    "df_train['LOS'] = (df_train['Discharge time'] - df_train['RE_DATE']).dt.days\n",
    "df_train['TOT_DAY'] = (df_train['Discharge time'] - df_train['Admission time']).dt.days\n",
    "\n",
    "# only reserve y-m-d for `RE_DATE` column\n",
    "df_train['RE_DATE'] = df_train['RE_DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge lab tests of the same (patient_id, date)\n",
    "df_train = df_train.groupby(['PATIENT_ID', 'RE_DATE'], dropna=True, as_index = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 2 types of prediction tasks: 1) predict mortality outcome, 2) length of stay\n",
    "\n",
    "# below are all lab test features\n",
    "labtest_features_str = \"\"\"\n",
    "Hypersensitive cardiac troponinI\themoglobin\tSerum chloride\tProthrombin time\tprocalcitonin\teosinophils(%)\tInterleukin 2 receptor\tAlkaline phosphatase\talbumin\tbasophil(%)\tInterleukin 10\tTotal bilirubin\tPlatelet count\tmonocytes(%)\tantithrombin\tInterleukin 8\tindirect bilirubin\tRed blood cell distribution width \tneutrophils(%)\ttotal protein\tQuantification of Treponema pallidum antibodies\tProthrombin activity\tHBsAg\tmean corpuscular volume\thematocrit\tWhite blood cell count\tTumor necrosis factorα\tmean corpuscular hemoglobin concentration\tfibrinogen\tInterleukin 1β\tUrea\tlymphocyte count\tPH value\tRed blood cell count\tEosinophil count\tCorrected calcium\tSerum potassium\tglucose\tneutrophils count\tDirect bilirubin\tMean platelet volume\tferritin\tRBC distribution width SD\tThrombin time\t(%)lymphocyte\tHCV antibody quantification\tD-D dimer\tTotal cholesterol\taspartate aminotransferase\tUric acid\tHCO3-\tcalcium\tAmino-terminal brain natriuretic peptide precursor(NT-proBNP)\tLactate dehydrogenase\tplatelet large cell ratio \tInterleukin 6\tFibrin degradation products\tmonocytes count\tPLT distribution width\tglobulin\tγ-glutamyl transpeptidase\tInternational standard ratio\tbasophil count(#)\t2019-nCoV nucleic acid detection\tmean corpuscular hemoglobin \tActivation of partial thromboplastin time\tHypersensitive c-reactive protein\tHIV antibody quantification\tserum sodium\tthrombocytocrit\tESR\tglutamic-pyruvic transaminase\teGFR\tcreatinine\n",
    "\"\"\"\n",
    "\n",
    "# below are 2 demographic features\n",
    "demographic_features_str = \"\"\"\n",
    "age\tgender\n",
    "\"\"\"\n",
    "\n",
    "labtest_features = [f for f in labtest_features_str.strip().split('\\t')]\n",
    "demographic_features = [f for f in demographic_features_str.strip().split('\\t')]\n",
    "target_features = ['outcome', 'LOS']\n",
    "\n",
    "# from our observation, `2019-nCoV nucleic acid detection` feature (in lab test) are all -1 value\n",
    "# so we remove this feature here\n",
    "labtest_features.remove('2019-nCoV nucleic acid detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features' statistics information\n",
    "\n",
    "def calculate_statistic_info(df, features):\n",
    "    \"\"\"all values calculated\"\"\"\n",
    "    statistic_info = {}\n",
    "    len_df = len(df)\n",
    "    for _, e in enumerate(features):\n",
    "        h = {}\n",
    "        h['count'] = int(df[e].count())\n",
    "        h['missing'] = str(round(float((100-df[e].count()*100/len_df)),3))+\"%\"\n",
    "        h['mean'] = float(df[e].mean())\n",
    "        h['max'] = float(df[e].max())\n",
    "        h['min'] = float(df[e].min())\n",
    "        h['median'] = float(df[e].median())\n",
    "        h['std'] = float(df[e].std())\n",
    "        statistic_info[e] = h\n",
    "    return statistic_info\n",
    "\n",
    "def calculate_middle_part_statistic_info(df, features):\n",
    "    \"\"\"calculate 5% ~ 95% percentile data\"\"\"\n",
    "    statistic_info = {}\n",
    "    len_df = len(df)\n",
    "    # calculate 5% and 95% percentile of dataframe\n",
    "    middle_part_df_info = df.quantile([.05, .95])\n",
    "\n",
    "    for _, e in enumerate(features):\n",
    "        low_value = middle_part_df_info[e][.05]\n",
    "        high_value = middle_part_df_info[e][.95]\n",
    "        middle_part_df_element = df.loc[(df[e] >= low_value) & (df[e] <= high_value)][e]\n",
    "        h = {}\n",
    "        h['count'] = int(middle_part_df_element.count())\n",
    "        h['missing'] = str(round(float((100-middle_part_df_element.count()*100/len_df)),3))+\"%\"\n",
    "        h['mean'] = float(middle_part_df_element.mean())\n",
    "        h['max'] = float(middle_part_df_element.max())\n",
    "        h['min'] = float(middle_part_df_element.min())\n",
    "        h['median'] = float(middle_part_df_element.median())\n",
    "        h['std'] = float(middle_part_df_element.std())\n",
    "        statistic_info[e] = h\n",
    "    return statistic_info\n",
    "\n",
    "# labtest_statistic_info = calculate_statistic_info(df_train, labtest_features)\n",
    "\n",
    "# group by patient_id, then calculate lab test/demographic features' statistics information\n",
    "groupby_patientid_df = df_train.groupby(['PATIENT_ID'], dropna=True, as_index = False).mean()\n",
    "\n",
    "# calculate statistic info (all values calculated)\n",
    "labtest_patientwise_statistic_info = calculate_statistic_info(groupby_patientid_df, labtest_features)\n",
    "demographic_statistic_info = calculate_statistic_info(groupby_patientid_df, demographic_features) # it's also patient-wise\n",
    "\n",
    "# calculate statistic info (5% ~ 95% only)\n",
    "demographic_statistic_info_2 = calculate_middle_part_statistic_info(groupby_patientid_df, demographic_features) \n",
    "labtest_patientwise_statistic_info_2 = calculate_middle_part_statistic_info(groupby_patientid_df, labtest_features) \n",
    "\n",
    "# take 2 statistics information's union\n",
    "statistic_info = labtest_patientwise_statistic_info_2 | demographic_statistic_info_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe features, export to csv file [optional]\n",
    "to_export_dict = {'name': [], 'missing_rate': [], 'count': [], 'mean': [], 'max': [], 'min': [], 'median': [], 'std': []}\n",
    "for key in statistic_info:\n",
    "    detail = statistic_info[key]\n",
    "    to_export_dict['name'].append(key)\n",
    "    to_export_dict['count'].append(detail['count'])\n",
    "    to_export_dict['missing_rate'].append(detail['missing'])\n",
    "    to_export_dict['mean'].append(detail['mean'])\n",
    "    to_export_dict['max'].append(detail['max'])\n",
    "    to_export_dict['min'].append(detail['min'])\n",
    "    to_export_dict['median'].append(detail['median'])\n",
    "    to_export_dict['std'].append(detail['std'])\n",
    "to_export_df = pd.DataFrame.from_dict(to_export_dict)\n",
    "# to_export_df.to_csv('statistic_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "def normalize_data(df, features, statistic_info):\n",
    "    df_features = df[features]\n",
    "    df_features = df_features.apply(lambda x: (x - statistic_info[x.name]['mean']) / (statistic_info[x.name]['std']+1e-12))\n",
    "    df = pd.concat([df[['PATIENT_ID', 'RE_DATE', 'outcome', 'LOS']], df_features], axis=1)\n",
    "    return df\n",
    "df_train = normalize_data(df_train, demographic_features + labtest_features, statistic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter outliers\n",
    "def filter_data(df, features, bar=3):\n",
    "    for f in features:\n",
    "        df[f] = df[f].mask(df[f].abs().gt(bar))\n",
    "    return df\n",
    "df_train = filter_data(df_train, demographic_features + labtest_features, bar=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_existing_length(data):\n",
    "    res = 0\n",
    "    for i in data:\n",
    "        if not pd.isna(i):\n",
    "            res += 1\n",
    "    return res\n",
    "# elements in data are sorted in time ascending order\n",
    "def fill_missing_value(data, to_fill_value=0):\n",
    "    data_len = len(data)\n",
    "    data_exist_len = calculate_data_existing_length(data)\n",
    "    if data_len == data_exist_len:\n",
    "        return data\n",
    "    elif data_exist_len == 0:\n",
    "        # data = [to_fill_value for _ in range(data_len)]\n",
    "        for i in range(data_len):\n",
    "            data[i] = to_fill_value\n",
    "        return data\n",
    "    if pd.isna(data[0]):\n",
    "        # find the first non-nan value's position\n",
    "        not_na_pos = 0\n",
    "        for i in range(data_len):\n",
    "            if not pd.isna(data[i]):\n",
    "                not_na_pos = i\n",
    "                break\n",
    "        # fill element before the first non-nan value\n",
    "        for i in range(not_na_pos):\n",
    "            data[i] = data[not_na_pos]\n",
    "    # fill element after the first non-nan value\n",
    "    for i in range(1, data_len):\n",
    "        if pd.isna(data[i]):\n",
    "            data[i] = data[i-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data using our strategy and convert to time series records\n",
    "grouped = df_train.groupby('PATIENT_ID')\n",
    "\n",
    "all_x_demographic = []\n",
    "all_x_labtest = []\n",
    "all_y = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    sorted_group = group.sort_values(by=['RE_DATE'], ascending=True)\n",
    "    patient_demographic = []\n",
    "    patient_labtest = []\n",
    "    patient_y = []\n",
    "    for f in demographic_features+labtest_features:\n",
    "        to_fill_value = (statistic_info[f]['median'] - statistic_info[f]['mean'])/(statistic_info[f]['std']+1e-12)\n",
    "        # take median patient as the default to-fill missing value\n",
    "        # print(sorted_group[f].values)\n",
    "        fill_missing_value(sorted_group[f].values, to_fill_value)\n",
    "        # print(sorted_group[f].values)\n",
    "        # print('-----------')\n",
    "    for _, v in sorted_group.iterrows():\n",
    "        patient_y.append([v['outcome'], v['LOS']])\n",
    "        demo = []\n",
    "        lab = []\n",
    "        for f in demographic_features:\n",
    "            demo.append(v[f])\n",
    "        for f in labtest_features:\n",
    "            lab.append(v[f])\n",
    "        patient_labtest.append(lab)\n",
    "        patient_demographic.append(demo)\n",
    "    all_y.append(patient_y)\n",
    "    all_x_demographic.append(patient_demographic[-1])\n",
    "    all_x_labtest.append(patient_labtest)\n",
    "\n",
    "# all_x_demographic (2 dim, record each patients' demographic features)\n",
    "# all_x_labtest (3 dim, record each patients' lab test features)\n",
    "# all_y (3 dim, patients' outcome/los of all visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_labtest = np.array(all_x_labtest, dtype=object)\n",
    "x_lab_length = [len(_) for _ in all_x_labtest]\n",
    "x_lab_length = torch.tensor(x_lab_length, dtype=torch.int)\n",
    "max_length = int(x_lab_length.max())\n",
    "all_x_labtest = [torch.tensor(_) for _ in all_x_labtest]\n",
    "# pad lab test sequence to the same shape\n",
    "all_x_labtest = torch.nn.utils.rnn.pad_sequence((all_x_labtest), batch_first=True)\n",
    "\n",
    "all_x_demographic = torch.tensor(all_x_demographic)\n",
    "batch_size, demo_dim = all_x_demographic.shape\n",
    "# repeat demographic tensor\n",
    "all_x_demographic = torch.reshape(all_x_demographic.repeat(1, max_length), (batch_size, max_length, demo_dim))\n",
    "# demographic tensor concat with lab test tensor\n",
    "all_x = torch.cat((all_x_demographic, all_x_labtest), 2)\n",
    "\n",
    "all_y = np.array(all_y, dtype=object)\n",
    "all_y = [torch.Tensor(_) for _ in all_y]\n",
    "# pad [outcome/los] sequence as well\n",
    "all_y = torch.nn.utils.rnn.pad_sequence((all_y), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickle format dataset (export torch tensor)\n",
    "pd.to_pickle(all_x, f'./processed_data/x.pkl')\n",
    "pd.to_pickle(all_y, f'./processed_data/y.pkl')\n",
    "pd.to_pickle(x_lab_length, f'./processed_data/visits_length.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(x):\n",
    "    if np.isnan(np.sum(x.cpu().numpy())):\n",
    "        print(\"some values from input are nan\")\n",
    "    else:\n",
    "        print(\"no nan\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e382889b16d65b8f9d2caeea05d88db6d501b8794eac9af8ee0956d5affe33e5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
